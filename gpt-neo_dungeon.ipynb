{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-neo dungeon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtMW21T9pDl"
      },
      "source": [
        "# Instructions\n",
        "**This notebook contains two custom finetuned models called gpt-neo-2.7B-horni and gpt-neo-2.7B-horni-ln which are NOT official EleutherAI models or affiliated with them in any way beyond using their model as a basis. You can also select the original EleutherAI/gpt-neo-2.7B model in the model selection's dropdown menu.**\n",
        "\n",
        "Go through each cell in this notebook one by one, take a look at the options and descriptions and then press the play button to the left of it. You can skip the optional one. Don't skip any of the others. After running the \"Play\" cell, a small form will appear underneath, which you can use to actually play.\n",
        "\n",
        "To reset the state of your game, run the \"Setup\" cell again. Closing the notebook will lose your progress, so if you want to keep your story, use the \"history\" action, copy out your story to a text editor. You can also copy out your author's note and memory from the output of the \"info\" action.\n",
        "\n",
        "The most reliable way of loading the models is to store them in your google drive. This notebook will automatically download the models from a google drive in the model setup step. If this succeeds, you can then copy it into your drive in the optional following step. You can also download the files yourself and upload them to your drive yourself.\n",
        "\n",
        "* [gpt-neo-2.7B-horni](https://mega.nz/file/6BNykLJb#B6gxK3TnCKBpeOF1DJMXwaLc_gcTcqMS0Lhzr1SeJmc) [[Google Drive](https://drive.google.com/file/d/1-Jj_hlyNCQxuSnK7FFBXREGnRSMI5MoF/view?usp=sharing)] 5GB, for NSFW styled output\n",
        "* [gpt-neo-2.7B-horni-ln](https://mega.nz/file/rQcWCTZR#tCx3Ztf_PMe6OtfgI95KweFT5fFTcMm7Nx9Jly_0wpg) [[Google Drive](https://drive.google.com/file/d/1M1JY459RBIgLghtWDRDXlD4Z5DAjjMwg/view?usp=sharing)] 5GB, for light novel styled output\n",
        "* Torrent: magnet:?xt=urn:btih:31d956ff4a248dcf914b1b7e474cbac02d70d6a4&dn=gtp-neo-horni\n",
        "\n",
        "Anon says about Google Drive: If you run into quotas, create a folder in your own google drive, add a shortcut to the file in question in said folder, and then download the folder itself instead of the file directly.\n",
        "\n",
        "If you have an RTX card with 8GB or more, you may be able to run this notebook locally. If you integrate the models in clover edition, use the transformers fork that's installed in the setup cell of this notebook to avoid OOM errors and use model.generate() to be able to use repetition_penalty_range and repetition_penalty_slope. For running locally, remove the map_device arguments from torch.load calls.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucP5hOdoMgzC",
        "cellView": "form"
      },
      "source": [
        "#@title Setup\n",
        "#@markdown Run this for setting up dependencies or resetting actions\n",
        "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-dungeon-localattention1\n",
        "!wget -c http://ftp.us.debian.org/debian/pool/main/m/megatools/megatools_1.11.0~git20200404-1_amd64.deb -O megatools.deb\n",
        "!dpkg -i megatools.deb\n",
        "!pip install gdown\n",
        "!nvidia-smi\n",
        "\n",
        "import os\n",
        "\n",
        "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
        "import tarfile\n",
        "import codecs\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "\n",
        "try:\n",
        "  initialized += 1\n",
        "except:\n",
        "  get_ipython().events.register('pre_run_cell', set_css)\n",
        "  initialized = 0\n",
        "\n",
        "actions = []\n",
        "memory = (\"\", torch.zeros((1, 0)).long())\n",
        "lmi = [\"\", torch.zeros((1, 0)).long()]\n",
        "an = (\"\", torch.zeros((1, 0)).long())\n",
        "an_depth = 3\n",
        "history = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ZF4eCFMV6E",
        "cellView": "form"
      },
      "source": [
        "#@title Model setup\n",
        "#@markdown horni was finetuned for one epoch on about 800MB worth of random blocks of text from the one dataset distributed by EleutherAI that is excluded from the pile dataset. Do not use the horni model if you dislike NSFW outputs. horni-ln uses horni as a base and was finetuned for one epoch on 579MB of text from a light novel dataset.\n",
        "\n",
        "print(\"Setting up model, this will take a few minutes\")\n",
        "\n",
        "model_name = \"gpt-neo-2.7B-horni\" #@param [\"gpt-neo-2.7B-horni\", \"gpt-neo-2.7B-horni-ln\", \"EleutherAI/gpt-neo-2.7B\"]\n",
        "model_gdrive = \"/content/drive/MyDrive/gpt-neo/gpt-neo-2.7B-horni.tar\" #@param {type:\"string\"}\n",
        "use_gdrive = True #@param {type:\"boolean\"}\n",
        "#@markdown If you download errors, the google drive downloads might be over their daily download quota. In that case, right-click, select \"interrupt execution\", download the checkpoint from mega yourself, upload to your google drive, tick use_gdrive and put the correct filename, e.g. `gpt-neo-2.7B-horni-ln.tar` and restart the cell.\n",
        "\n",
        "custom_models = [\"gpt-neo-2.7B-horni\", \"gpt-neo-2.7B-horni-ln\"]\n",
        "\n",
        "if use_gdrive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "#model_types = {\"gpt-neo-2.7B-horni\": \"https://mega.nz/file/6BNykLJb#B6gxK3TnCKBpeOF1DJMXwaLc_gcTcqMS0Lhzr1SeJmc\",\n",
        "#               \"gpt-neo-2.7B-horni-ln\": \"https://mega.nz/file/rQcWCTZR#tCx3Ztf_PMe6OtfgI95KweFT5fFTcMm7Nx9Jly_0wpg\"}\n",
        "model_types = {\"gpt-neo-2.7B-horni\": \"https://drive.google.com/uc?id=1-Jj_hlyNCQxuSnK7FFBXREGnRSMI5MoF\",\n",
        "               \"gpt-neo-2.7B-horni-ln\": \"https://drive.google.com/uc?id=1M1JY459RBIgLghtWDRDXlD4Z5DAjjMwg\"}\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "pipeline = None\n",
        "checkpoint = None\n",
        "\n",
        "if not os.path.isdir(model_name) and model_name in custom_models:\n",
        "  if use_gdrive:\n",
        "    tar = tarfile.open(model_gdrive, \"r\")\n",
        "  else:\n",
        "    model_url = model_types[model_name]\n",
        "    print(\"Downloading:\", model_url)\n",
        "    #!megadl $model_url --no-ask-password\n",
        "    !gdown $model_url\n",
        "    tar = tarfile.open(model_name + \".tar\", \"r\")\n",
        "  tar.extractall()\n",
        "  tar.close()\n",
        "\n",
        "if model_name in custom_models:\n",
        "  checkpoint = torch.load(model_name + \"/pytorch_model.bin\", map_location=\"cuda:0\")\n",
        "  model = GPTNeoForCausalLM.from_pretrained(model_name, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "  for k in list(checkpoint.keys()):\n",
        "    del checkpoint[k]\n",
        "  del checkpoint\n",
        "else:\n",
        "  from transformers.file_utils import cached_path, WEIGHTS_NAME, hf_bucket_url\n",
        "  archive_file = hf_bucket_url(model_name, filename=WEIGHTS_NAME)\n",
        "  resolved_archive_file = cached_path(archive_file)\n",
        "  checkpoint = torch.load(resolved_archive_file, map_location=\"cuda:0\")\n",
        "  for k in checkpoint.keys():\n",
        "    checkpoint[k] = checkpoint[k].half()\n",
        "  model = GPTNeoForCausalLM.from_pretrained(model_name, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "  for k in list(checkpoint.keys()):\n",
        "    del checkpoint[k]\n",
        "  del checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZFSWPEP1U2b",
        "cellView": "form"
      },
      "source": [
        "#@title Copy downloaded model to google drive (optional)\n",
        "#@markdown If the model checkpoint was downloaded automatically in the previous step, you can copy it to your google drive here for more reliable access in the future\n",
        "gdrive_target = \"/content/drive/MyDrive/gpt-neo-2.7B-horni.tar\" #@param {type:\"string\"}\n",
        "copy_model_file = False #@param {type:\"boolean\"}\n",
        "\n",
        "if copy_model_file:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  model_tar = '/content/' + model_name + \".tar\"\n",
        "  !cp -v $model_tar $gdrive_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls_x_sSLN2hU",
        "cellView": "form"
      },
      "source": [
        "#@title Sampling settings\n",
        "#@markdown You can modify sampling settings here. Don't forget to run the cell again after changing. The number of generated tokens is subtracted from the context window size, don't set it high.\n",
        "top_k = 60 #@param {type:\"number\"}\n",
        "top_p = 0.9 #@param {type:\"number\"}\n",
        "temperature =  0.9#@param {type:\"number\"}\n",
        "number_generated_tokens =  100#@param {type:\"integer\"}\n",
        "repetition_penalty = 1.75 #@param {type:\"number\"}\n",
        "repetition_penalty_range = 300 #@param {type:\"number\"}\n",
        "repetition_penalty_slope = 3.80 #@param {type:\"number\"}\n",
        "number_show_last_actions = 15 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Temperatures seem to give results different from those in AID, so play around with it. Even 0.5 can give good results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "qd-HLb2nXaaA"
      },
      "source": [
        "#@title Basic sampling\n",
        "\n",
        "#@markdown Use this cell if you just want to sample from the model in a free form way.\n",
        "\n",
        "basic_prompt = \"The rays of the evening sun falling in through the window bathed the room in a soft, warm light\" #@param {type:\"string\"}\n",
        "\n",
        "ids = tokenizer(basic_prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "n_ids = ids.shape[1]\n",
        "if n_ids < 1:\n",
        "  n_ids = 1\n",
        "  ids = torch.tensor([[tokenizer.eos_token_id]])\n",
        "max_length = n_ids + number_generated_tokens\n",
        "torch.cuda.empty_cache()\n",
        "basic_output = model.generate(\n",
        "    ids.long().cuda(),\n",
        "    do_sample=True,\n",
        "    min_length=max_length,\n",
        "    max_length=max_length,\n",
        "    temperature=temperature,\n",
        "    top_k = top_k,\n",
        "    top_p = top_p,\n",
        "    repetition_penalty = repetition_penalty,\n",
        "    repetition_penalty_range = repetition_penalty_range,\n",
        "    repetition_penalty_slope = repetition_penalty_slope,\n",
        "    use_cache=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ").long().to(\"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(tokenizer.decode(basic_output[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GKfy1uQ9QdU"
      },
      "source": [
        "# Using gpt-neo dungeon's play function\n",
        "\n",
        "If your prompt starts with a letter, try putting a space or newline in front.\n",
        "\n",
        "* **generate** adds your prompt as an action and generates more output\n",
        "* **continue** generates more output\n",
        "* **edit** copies the last action into the prompt field and sets action to replace\n",
        "* **replace** replaces the last output with the prompt and generates more, use this to edit\n",
        "* **info** outputs LMI and memory\n",
        "* **history** outputs all actions so far\n",
        "* **memory** sets memory to the text in the prompt field\n",
        "* **authorsnote** sets author's note to the text in the prompt field\n",
        "* **andepth** sets the depth of the author's note to the number in the prompt\n",
        "* **tokenize** tokenizes the text in the prompt field and outputs the number of tokens"
        "\n"
        "###Helpful websites"
        "\n"
        "Zalty's writing style list: https://justpaste.it/9ofj1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb_NHHISOeYg",
        "cellView": "form"
      },
      "source": [
        "#@title Play\n",
        "\n",
        "action_type = \"generate\"\n",
        "prompt = \"\"\n",
        "need_refresh = True\n",
        "\n",
        "action_types = [\"generate\", \"continue\", \"edit\", \"replace\", \"undo\", \"retry\", \"memory\", \"authorsnote\", \"andepth\", \"info\", \"history\", \"tokenize\"]\n",
        "\n",
        "def assemble():\n",
        "  remaining = (2048 - number_generated_tokens + 1) - memory[1].shape[1] - an[1].shape[1]\n",
        "  n_actions = len(actions)\n",
        "  n_ctx = 0\n",
        "  back_i = n_actions\n",
        "  for i in range(n_actions):\n",
        "      i_action = n_actions - i - 1\n",
        "      n_tok = actions[i_action][1].shape[1]\n",
        "      if remaining > n_ctx + n_tok:\n",
        "        n_ctx += n_tok\n",
        "        back_i = i_action\n",
        "      else:\n",
        "        break\n",
        "  lmi[0], lmi[1] = memory[0], memory[1]\n",
        "  start = False\n",
        "  if n_actions - back_i - 1 < an_depth:\n",
        "    start = True\n",
        "  while back_i < n_actions:\n",
        "    if start or n_actions - back_i - 1 == an_depth:\n",
        "      lmi[0] += an[0]\n",
        "      lmi[1] = torch.cat([lmi[1].cpu(), an[1].cpu()], 1).long()\n",
        "      start = False\n",
        "    lmi[0] += actions[back_i][0]\n",
        "    lmi[1] = torch.cat([lmi[1].cpu(), actions[back_i][1].cpu()], 1).long()\n",
        "    back_i += 1\n",
        "\n",
        "def clear_output():\n",
        "  with out:\n",
        "    IPython.display.clear_output()\n",
        "\n",
        "def set_action(change):\n",
        "  global action_type\n",
        "  action_type = change.new\n",
        "\n",
        "def set_prompt(change):\n",
        "  global prompt\n",
        "  prompt = change.new\n",
        "\n",
        "@torch.no_grad()\n",
        "def play(do_action=None):\n",
        "  global memory, need_refresh, an, an_depth, action_type, history\n",
        "  an_updated = False\n",
        "  memory_updated = False\n",
        "  if do_action is not None:\n",
        "    action = do_action\n",
        "    action_type = do_action\n",
        "  else:\n",
        "    action = action_type\n",
        "  with out:\n",
        "    if prompt in action_types:\n",
        "      action == prompt\n",
        "    else:\n",
        "      if action == \"edit\":\n",
        "        if len(actions) > 0:\n",
        "          input.value = actions[-1][0]\n",
        "        else:\n",
        "          input.value = \"\"\n",
        "        dropdown.value = \"replace\"\n",
        "        return\n",
        "      if action == \"replace\":\n",
        "        if len(actions) > 0:\n",
        "          actions.pop()\n",
        "        need_refresh = True\n",
        "        action = \"generate\"\n",
        "      if action == \"generate\":\n",
        "        text = prompt\n",
        "        if len(text) > 0:\n",
        "          for line in text.splitlines(True):\n",
        "            tokens = tokenizer(line, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "            actions.append((line, tokens))\n",
        "        action = \"continue\"\n",
        "      if action == \"info\":\n",
        "        clear_output()\n",
        "        print(\"LMI: \" + lmi[0])\n",
        "        print(\"LMI tokens: \" + str(lmi[1].shape[1]))\n",
        "        print(\"Memory: \" + memory[0])\n",
        "        print(\"Author's note: \" + an[0])\n",
        "        print(\"Author's note depth: \" + str(an_depth))\n",
        "        need_refresh = True\n",
        "      if action == \"history\":\n",
        "        clear_output()\n",
        "        print(\"\".join([action[0] for action in actions]), end=\"\")\n",
        "        need_refresh = False\n",
        "      if action == \"retry\":\n",
        "        if len(actions) > 0:\n",
        "          actions.pop()\n",
        "        need_refresh = True\n",
        "        action = \"continue\"\n",
        "      if action == \"undo\":\n",
        "        if len(actions) > 0:\n",
        "          actions.pop()\n",
        "        assemble()\n",
        "        clear_output()\n",
        "        print(\"\".join([action[0] for action in actions[-number_show_last_actions:]]), end=\"\")\n",
        "        need_refresh = False\n",
        "      if action == \"memory\":\n",
        "        if prompt == \"\":\n",
        "          memory = (\"\", torch.zeros((1, 0)).long())\n",
        "          text = \"\"\n",
        "        else:\n",
        "          text = codecs.decode(prompt + \"\\n\", \"unicode-escape\")\n",
        "          tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "          memory = (text, tokens)\n",
        "        clear_output()\n",
        "        print(\"Memory: \" + text)\n",
        "        memory_updated = True\n",
        "      if action == \"authorsnote\":\n",
        "        if prompt == \"\":\n",
        "          an = (\"\", torch.zeros((1, 0)).long())\n",
        "          text = \"\"\n",
        "        else:\n",
        "          text = \"\\n[Author's note: \" + codecs.decode(prompt, \"unicode-escape\") + \"]\\n\"\n",
        "          tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "          an = (text, tokens)\n",
        "        clear_output()\n",
        "        print(\"Author's note: \" + text)\n",
        "        an_updated = True\n",
        "      if action == \"andepth\":\n",
        "        clear_output()\n",
        "        try:\n",
        "          an_depth = int(codecs.decode(prompt + \"\\n\", \"unicode-escape\"))\n",
        "        except:\n",
        "          pass\n",
        "        print(\"Author's note depth: \" + str(an_depth))\n",
        "        an_updated = True\n",
        "      if action == \"tokenize\":\n",
        "        text = codecs.decode(prompt, \"unicode-escape\")\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "        clear_output()\n",
        "        print(\"Tokens: \" + str(tokens.shape[1]))\n",
        "        print(tokens[0])\n",
        "        need_refresh = True\n",
        "      if action == \"continue\":\n",
        "        assemble()\n",
        "        ids = lmi[1].cuda()\n",
        "        n_ids = ids.shape[1]\n",
        "        if n_ids < 1:\n",
        "          n_ids = 1\n",
        "          ids = torch.tensor([[tokenizer.eos_token_id]])\n",
        "        max_length = number_generated_tokens + n_ids\n",
        "        #ids[:, :] = 13\n",
        "        torch.cuda.empty_cache()\n",
        "        clear_output()\n",
        "        gen_tokens = model.generate(\n",
        "            ids.long().cuda(),\n",
        "            do_sample=True,\n",
        "            min_length=max_length,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_k = top_k,\n",
        "            top_p = top_p,\n",
        "            repetition_penalty = repetition_penalty,\n",
        "            repetition_penalty_range = repetition_penalty_range,\n",
        "            repetition_penalty_slope = repetition_penalty_slope,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        ).long()\n",
        "        stop_tokens = [0, 13, 30, 526, 764, 1701, 2474, 5145, 5633]\n",
        "        for i in reversed(range(len(gen_tokens[0]))):\n",
        "          if i < n_ids:\n",
        "            gen_tokens = gen_tokens[0]\n",
        "            break\n",
        "          if gen_tokens[0][i] in stop_tokens:\n",
        "            gen_tokens = gen_tokens[0][:i+1]\n",
        "            break\n",
        "        gen_text = tokenizer.decode(gen_tokens[n_ids:])\n",
        "        if len(gen_text) > 0:\n",
        "          actions.append((gen_text, gen_tokens[n_ids:].unsqueeze(0).cpu()))\n",
        "        print(\"\".join([action[0] for action in actions[-number_show_last_actions:]]), end=\"\")\n",
        "        torch.cuda.empty_cache()\n",
        "        need_refresh = False\n",
        "    if history is not None:\n",
        "      if history:\n",
        "        with out_history:\n",
        "          IPython.display.clear_output()\n",
        "          print(\"\".join([action[0] for action in actions]), end=\"\")\n",
        "        with out_history2:\n",
        "          IPython.display.clear_output()\n",
        "      else:\n",
        "        with out_history2:\n",
        "          IPython.display.clear_output()\n",
        "          print(\"\".join([action[0] for action in actions]), end=\"\")\n",
        "        with out_history:\n",
        "          IPython.display.clear_output()\n",
        "      if an_updated:\n",
        "        with out_an:\n",
        "          IPython.display.clear_output()\n",
        "          if len(an[0]) > 0:\n",
        "            print(\"AN depth: \" + str(an_depth) + \"\\n\" + an[0], end=\"\")\n",
        "      if memory_updated:\n",
        "        with out_memory:\n",
        "          IPython.display.clear_output()\n",
        "          print(memory[0], end=\"\")\n",
        "      history = not history\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import IPython.display\n",
        "out = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "dropdown = widgets.Dropdown(options=action_types, value=action_type, description='Action:', disabled=False)\n",
        "dropdown.observe(set_action, 'value')\n",
        "button = widgets.Button(description='[selected action]', disabled=False)\n",
        "button.on_click(lambda _: play(dropdown.value))\n",
        "edit_button = widgets.Button(description='Edit', disabled=False)\n",
        "edit_button.on_click(lambda _: play(\"edit\"))\n",
        "retry_button = widgets.Button(description='Retry', disabled=False)\n",
        "retry_button.on_click(lambda _: play(\"retry\"))\n",
        "continue_button = widgets.Button(description='Continue', disabled=False)\n",
        "continue_button.on_click(lambda _: play(\"continue\"))\n",
        "undo_button = widgets.Button(description='Undo', disabled=False)\n",
        "undo_button.on_click(lambda _: play(\"undo\"))\n",
        "hbox = widgets.HBox([button, edit_button, retry_button, continue_button, undo_button])\n",
        "input = widgets.Textarea(value='', placeholder='', description='Input:', disabled=False, rows=4, layout={\"width\": \"1280px\"})\n",
        "input.observe(set_prompt, 'value')\n",
        "\n",
        "display(out, dropdown, hbox, input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jPtYoUNO39Mg"
      },
      "source": [
        "#@title History\n",
        "#@markdown Run this cell to have an auto-updating full listing of the current story.\n",
        "\n",
        "history = True\n",
        "out_history = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "out_history2 = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "out_memory = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "out_an = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "display(out_history, out_history2, out_memory, out_an)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
